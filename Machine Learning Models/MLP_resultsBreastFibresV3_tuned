#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Feb  5 16:47:04 2025

@author: amrarafat
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, make_scorer

# Load the Data
df = pd.read_csv("/Users/amrarafat/Documents/MachineLearning/results_Breast_FibresV3.csv")

# Preparing the Data
feature_columns = [
    "numEdges","avOrientation","stdOrientation","avMajorAxisLength","avMinorAxisLength","stdMajorAxisLength",
    "stdMinorAxisLength","gapArea","gapAreaRel","largestGap", "areaTakenFibres","avMajorAxisRel","avMinorAxisRel",
    "MinAxMajAx","avOrientation5","stdOrientation5","meanIntensity","stdIntensity","numRegions","areaFibres",
    "avgCircularity","stdCircularity","avgCurvature","stdCurvature","avgEccentricity","stdEccentricity",
    "avgMeanIntensity2","stdMeanIntensity2","avgMinIntensity2","stdMinIntensity2","avgMaxIntensity2","stdMaxIntensity2"
]

X = df[feature_columns]
y = df['Classification'].map({'Normal': 0, 'TUM': 1})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the Data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the MLP Model
mlp = MLPClassifier(max_iter=500, random_state=42)

# Define the Parameter Grid for Optimization
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # Different neuron configurations
    'activation': ['relu', 'tanh', 'logistic'],  # Activation functions
    'solver': ['adam', 'sgd', 'lbfgs'],  # Optimizers
    'alpha': [0.0001, 0.001, 0.01, 0.1]  # Regularization strength (L2)
}

# Define Scoring Metrics
scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring=scoring, refit='accuracy', n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)

# Print Best Parameters and Accuracy
print("\nBest Parameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

# Print All Results
cv_results = grid_search.cv_results_

print("\nDetailed Results for Each Parameter Combination:")
for i in range(len(cv_results['params'])):
    print(f"Params: {cv_results['params'][i]}")
    print(f"  Accuracy: {cv_results['mean_test_accuracy'][i]:.4f}")
    print(f"  Precision: {cv_results['mean_test_precision'][i]:.4f}")
    print(f"  Recall: {cv_results['mean_test_recall'][i]:.4f}")
    print(f"  F1 Score: {cv_results['mean_test_f1'][i]:.4f}")
    print("-" * 50)

# Train the Best Model
best_mlp = grid_search.best_estimator_
y_pred = best_mlp.predict(X_test)

# Evaluate the Final Model
accuracy = accuracy_score(y_test, y_pred)
print("\nFinal Model Evaluation on Test Set:")
print(f'Optimized Accuracy: {accuracy:.4f}')
print(classification_report(y_test, y_pred, target_names=['Normal', 'TUM']))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'TUM'], yticklabels=['Normal', 'TUM'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix: MLP Fibres')
plt.show()
